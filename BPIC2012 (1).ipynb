{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "16SqPNMLnSrk",
    "outputId": "bd844960-de02-47d8-fe85-92087d521efa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Complete Timestamp</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Variant index</th>\n",
       "      <th>(case) AMOUNT_REQ</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_SUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:38:44.546</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:38:44.880</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PREACCEPTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:39:37.906</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-SCHEDULE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:39:38.875</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>SCHEDULE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011/10/01 18:36:46.437</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case ID                         Activity  Resource  \\\n",
       "0   173688             A_SUBMITTED-COMPLETE     112.0   \n",
       "1   173688       A_PARTLYSUBMITTED-COMPLETE     112.0   \n",
       "2   173688           A_PREACCEPTED-COMPLETE     112.0   \n",
       "3   173688  W_Completeren aanvraag-SCHEDULE     112.0   \n",
       "4   173688     W_Completeren aanvraag-START       NaN   \n",
       "\n",
       "        Complete Timestamp      Variant  Variant index  (case) AMOUNT_REQ  \\\n",
       "0  2011/10/01 07:38:44.546  Variant 613            613              20000   \n",
       "1  2011/10/01 07:38:44.880  Variant 613            613              20000   \n",
       "2  2011/10/01 07:39:37.906  Variant 613            613              20000   \n",
       "3  2011/10/01 07:39:38.875  Variant 613            613              20000   \n",
       "4  2011/10/01 18:36:46.437  Variant 613            613              20000   \n",
       "\n",
       "             concept:name lifecycle:transition  \n",
       "0             A_SUBMITTED             COMPLETE  \n",
       "1       A_PARTLYSUBMITTED             COMPLETE  \n",
       "2           A_PREACCEPTED             COMPLETE  \n",
       "3  W_Completeren aanvraag             SCHEDULE  \n",
       "4  W_Completeren aanvraag                START  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import an event log as Dataframe\n",
    "\n",
    "file = pd.read_csv(r'C:\\Users\\Dias\\Desktop\\data\\dataset\\BPIC12.csv')\n",
    "raw_df = pd.DataFrame(file)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WjBs2QNcnSrq"
   },
   "outputs": [],
   "source": [
    "#df.to_csv('data/dataset/bpic/bpic2012/BPIC12_dur.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "BrDhNJ2DnSrr",
    "outputId": "38e5dbed-2ac0-4544-ea07-187c3a3296da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Complete Timestamp</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Variant index</th>\n",
       "      <th>(case) AMOUNT_REQ</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>Accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_SUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:38:44.546</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:38:44.880</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PREACCEPTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:39:37.906</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-SCHEDULE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:39:38.875</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011/10/01 18:36:46.437</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>START</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262195</th>\n",
       "      <td>214376</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2012/03/01 07:51:17.423</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262196</th>\n",
       "      <td>214376</td>\n",
       "      <td>W_Afhandelen leads-SCHEDULE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2012/03/01 07:52:01.287</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262197</th>\n",
       "      <td>214376</td>\n",
       "      <td>W_Afhandelen leads-START</td>\n",
       "      <td>11169.0</td>\n",
       "      <td>2012/03/01 17:26:46.736</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>START</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262198</th>\n",
       "      <td>214376</td>\n",
       "      <td>A_DECLINED-COMPLETE</td>\n",
       "      <td>11169.0</td>\n",
       "      <td>2012/03/01 17:27:37.118</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262199</th>\n",
       "      <td>214376</td>\n",
       "      <td>W_Afhandelen leads-COMPLETE</td>\n",
       "      <td>11169.0</td>\n",
       "      <td>2012/03/01 17:27:41.325</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262200 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Case ID                         Activity  Resource  \\\n",
       "0        173688             A_SUBMITTED-COMPLETE     112.0   \n",
       "1        173688       A_PARTLYSUBMITTED-COMPLETE     112.0   \n",
       "2        173688           A_PREACCEPTED-COMPLETE     112.0   \n",
       "3        173688  W_Completeren aanvraag-SCHEDULE     112.0   \n",
       "4        173688     W_Completeren aanvraag-START       NaN   \n",
       "...         ...                              ...       ...   \n",
       "262195   214376       A_PARTLYSUBMITTED-COMPLETE     112.0   \n",
       "262196   214376      W_Afhandelen leads-SCHEDULE     112.0   \n",
       "262197   214376         W_Afhandelen leads-START   11169.0   \n",
       "262198   214376              A_DECLINED-COMPLETE   11169.0   \n",
       "262199   214376      W_Afhandelen leads-COMPLETE   11169.0   \n",
       "\n",
       "             Complete Timestamp      Variant  Variant index  \\\n",
       "0       2011/10/01 07:38:44.546  Variant 613            613   \n",
       "1       2011/10/01 07:38:44.880  Variant 613            613   \n",
       "2       2011/10/01 07:39:37.906  Variant 613            613   \n",
       "3       2011/10/01 07:39:38.875  Variant 613            613   \n",
       "4       2011/10/01 18:36:46.437  Variant 613            613   \n",
       "...                         ...          ...            ...   \n",
       "262195  2012/03/01 07:51:17.423    Variant 2              2   \n",
       "262196  2012/03/01 07:52:01.287    Variant 2              2   \n",
       "262197  2012/03/01 17:26:46.736    Variant 2              2   \n",
       "262198  2012/03/01 17:27:37.118    Variant 2              2   \n",
       "262199  2012/03/01 17:27:41.325    Variant 2              2   \n",
       "\n",
       "        (case) AMOUNT_REQ            concept:name lifecycle:transition  \\\n",
       "0                   20000             A_SUBMITTED             COMPLETE   \n",
       "1                   20000       A_PARTLYSUBMITTED             COMPLETE   \n",
       "2                   20000           A_PREACCEPTED             COMPLETE   \n",
       "3                   20000  W_Completeren aanvraag             SCHEDULE   \n",
       "4                   20000  W_Completeren aanvraag                START   \n",
       "...                   ...                     ...                  ...   \n",
       "262195              15000       A_PARTLYSUBMITTED             COMPLETE   \n",
       "262196              15000      W_Afhandelen leads             SCHEDULE   \n",
       "262197              15000      W_Afhandelen leads                START   \n",
       "262198              15000              A_DECLINED             COMPLETE   \n",
       "262199              15000      W_Afhandelen leads             COMPLETE   \n",
       "\n",
       "        Accepted  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "262195         0  \n",
       "262196         0  \n",
       "262197         0  \n",
       "262198         0  \n",
       "262199         0  \n",
       "\n",
       "[262200 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Labeling\n",
    "\n",
    "label = []\n",
    "for case, group in raw_df.groupby('Case ID'):\n",
    "    if 'O_ACCEPTED' in group['concept:name'].tolist():\n",
    "        for i in range(len(group)):\n",
    "            label.append(1)\n",
    "    else:\n",
    "        for i in range(len(group)):\n",
    "            label.append(0)\n",
    "            \n",
    "label_df = pd.DataFrame(label, columns = ['Accepted'])\n",
    "raw_df = pd.concat([raw_df, label_df], axis=1)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "3l0IIUn6nSrs",
    "outputId": "e9c42af7-053c-4612-c6fe-833a133ccb8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Complete Timestamp</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Variant index</th>\n",
       "      <th>(case) AMOUNT_REQ</th>\n",
       "      <th>Accepted</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_SUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:38:44.546</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:38:44.880</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PREACCEPTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:39:37.906</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-SCHEDULE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 07:39:38.875</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011/10/01 18:36:46.437</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.952101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262195</th>\n",
       "      <td>214376</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2012/03/01 07:51:17.423</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262196</th>\n",
       "      <td>214376</td>\n",
       "      <td>W_Afhandelen leads-SCHEDULE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2012/03/01 07:52:01.287</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262197</th>\n",
       "      <td>214376</td>\n",
       "      <td>W_Afhandelen leads-START</td>\n",
       "      <td>11169.0</td>\n",
       "      <td>2012/03/01 17:26:46.736</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.579291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262198</th>\n",
       "      <td>214376</td>\n",
       "      <td>A_DECLINED-COMPLETE</td>\n",
       "      <td>11169.0</td>\n",
       "      <td>2012/03/01 17:27:37.118</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262199</th>\n",
       "      <td>214376</td>\n",
       "      <td>W_Afhandelen leads-COMPLETE</td>\n",
       "      <td>11169.0</td>\n",
       "      <td>2012/03/01 17:27:41.325</td>\n",
       "      <td>Variant 2</td>\n",
       "      <td>2</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Case ID                         Activity  Resource  \\\n",
       "0        173688             A_SUBMITTED-COMPLETE     112.0   \n",
       "1        173688       A_PARTLYSUBMITTED-COMPLETE     112.0   \n",
       "2        173688           A_PREACCEPTED-COMPLETE     112.0   \n",
       "3        173688  W_Completeren aanvraag-SCHEDULE     112.0   \n",
       "4        173688     W_Completeren aanvraag-START       NaN   \n",
       "...         ...                              ...       ...   \n",
       "262195   214376       A_PARTLYSUBMITTED-COMPLETE     112.0   \n",
       "262196   214376      W_Afhandelen leads-SCHEDULE     112.0   \n",
       "262197   214376         W_Afhandelen leads-START   11169.0   \n",
       "262198   214376              A_DECLINED-COMPLETE   11169.0   \n",
       "262199   214376      W_Afhandelen leads-COMPLETE   11169.0   \n",
       "\n",
       "             Complete Timestamp      Variant  Variant index  \\\n",
       "0       2011/10/01 07:38:44.546  Variant 613            613   \n",
       "1       2011/10/01 07:38:44.880  Variant 613            613   \n",
       "2       2011/10/01 07:39:37.906  Variant 613            613   \n",
       "3       2011/10/01 07:39:38.875  Variant 613            613   \n",
       "4       2011/10/01 18:36:46.437  Variant 613            613   \n",
       "...                         ...          ...            ...   \n",
       "262195  2012/03/01 07:51:17.423    Variant 2              2   \n",
       "262196  2012/03/01 07:52:01.287    Variant 2              2   \n",
       "262197  2012/03/01 17:26:46.736    Variant 2              2   \n",
       "262198  2012/03/01 17:27:37.118    Variant 2              2   \n",
       "262199  2012/03/01 17:27:41.325    Variant 2              2   \n",
       "\n",
       "        (case) AMOUNT_REQ  Accepted   duration  \n",
       "0                   20000         1   0.000000  \n",
       "1                   20000         1   0.000093  \n",
       "2                   20000         1   0.014729  \n",
       "3                   20000         1   0.000269  \n",
       "4                   20000         1  10.952101  \n",
       "...                   ...       ...        ...  \n",
       "262195              15000         0   0.000173  \n",
       "262196              15000         0   0.012184  \n",
       "262197              15000         0   9.579291  \n",
       "262198              15000         0   0.013995  \n",
       "262199              15000         0   0.001169  \n",
       "\n",
       "[262200 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add duration column\n",
    "\n",
    "df = raw_df.drop(['concept:name', 'lifecycle:transition'], axis = 1)\n",
    "\n",
    "duration = [0]\n",
    "for j in range(len(df)-1):\n",
    "    \n",
    "    if df.iloc[j]['Case ID'] == df.iloc[j+1]['Case ID']:\n",
    "        x = pd.to_datetime(df.iloc[j]['Complete Timestamp']).timestamp()\n",
    "        y = pd.to_datetime(df.iloc[j+1]['Complete Timestamp']).timestamp()\n",
    "        duration.append((y-x)/3600)\n",
    "    else:\n",
    "        duration.append(0)\n",
    "        \n",
    "ts_df = pd.DataFrame(duration, columns = ['duration'])\n",
    "df = pd.concat([df, ts_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ByNb_DinnSrt"
   },
   "outputs": [],
   "source": [
    "df = df[df['Activity']!='O_ACCEPTED-COMPLETE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NQnkqW_fnSrt"
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Dias\\Desktop\\data\\dataset\\BPIC12_dur.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOQaV5nOnSru"
   },
   "source": [
    "# prefixlength_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRI_muZGnSrw",
    "outputId": "982ac934-d1ea-4afe-e69a-522d0e184add",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:01<00:00, 11009.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 14387.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 15254.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 16136.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 17249.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 19016.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 20016.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13087/13087 [00:00<00:00, 24161.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "#from playsound import playsound\n",
    "import warnings\n",
    "\n",
    "\n",
    "def morethan(df,length):\n",
    "    #df['END_DATE'] = pd.to_datetime(df['END_DATE'])\n",
    "    groups = df.groupby('Case ID')\n",
    "    lenmorethan=[]\n",
    "    \n",
    "    for case, group in tqdm(groups):\n",
    "        #group = group.sort_values(by='remaining_time', ascending = False).reset_index(drop=True)\n",
    "        if len(group) >= length:\n",
    "            addgroup = group.iloc[:length,:]\n",
    "            #print(addgroup)\n",
    "            #activitylist = list(addgroup['ACTIVITY'])\n",
    "            lenmorethan.append(addgroup)\n",
    "    df = pd.concat(lenmorethan)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    for x in range(5, 41, 5):\n",
    "        length = x \n",
    "        df = pd.read_csv(r'C:\\Users\\Dias\\Desktop\\data\\dataset\\BPIC12_dur.csv')\n",
    "        df = morethan(df,length)\n",
    "        \n",
    "        dir_path = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\prefix'+str(length)\n",
    "        try:\n",
    "            os.makedirs(dir_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df.to_csv(dir_path+r'\\bpic2012_prep.csv',index=False)\n",
    "    #playsound('../Yattong edited version.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wd990FOPnSrx",
    "outputId": "e6bb3ed3-36d8-4603-c600-7050d8420601"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Complete Timestamp</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Variant index</th>\n",
       "      <th>(case) AMOUNT_REQ</th>\n",
       "      <th>Accepted</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>173694</td>\n",
       "      <td>A_SUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 15:10:30.287</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>173694</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 15:10:30.591</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>173694</td>\n",
       "      <td>A_PREACCEPTED-COMPLETE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 15:11:13.026</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-SCHEDULE</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011/10/01 15:11:13.390</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>10912.0</td>\n",
       "      <td>2011/10/01 18:31:25.301</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.336642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-COMPLETE</td>\n",
       "      <td>10912.0</td>\n",
       "      <td>2011/10/01 18:35:59.637</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011/10/03 18:12:02.445</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>47.600780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-COMPLETE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011/10/03 18:21:22.417</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:21:06.626</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.995614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:32:43.313</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:32:52.818</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>173694</td>\n",
       "      <td>A_ACCEPTED-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:34:25.421</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>173694</td>\n",
       "      <td>A_FINALIZED-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:14.632</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>173694</td>\n",
       "      <td>O_SELECTED-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:14.632</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>173694</td>\n",
       "      <td>O_CREATED-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:15.605</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>173694</td>\n",
       "      <td>O_SENT-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:15.651</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Nabellen offertes-SCHEDULE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:15.865</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Completeren aanvraag-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:17.062</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>173694</td>\n",
       "      <td>W_Nabellen offertes-START</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:40:42.469</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>173694</td>\n",
       "      <td>O_SELECTED-COMPLETE</td>\n",
       "      <td>11201.0</td>\n",
       "      <td>2011/10/03 20:44:17.619</td>\n",
       "      <td>Variant 615</td>\n",
       "      <td>615</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case ID                         Activity  Resource  \\\n",
       "63   173694             A_SUBMITTED-COMPLETE     112.0   \n",
       "64   173694       A_PARTLYSUBMITTED-COMPLETE     112.0   \n",
       "65   173694           A_PREACCEPTED-COMPLETE     112.0   \n",
       "66   173694  W_Completeren aanvraag-SCHEDULE     112.0   \n",
       "67   173694     W_Completeren aanvraag-START   10912.0   \n",
       "68   173694  W_Completeren aanvraag-COMPLETE   10912.0   \n",
       "69   173694     W_Completeren aanvraag-START       NaN   \n",
       "70   173694  W_Completeren aanvraag-COMPLETE       NaN   \n",
       "71   173694     W_Completeren aanvraag-START   11201.0   \n",
       "72   173694  W_Completeren aanvraag-COMPLETE   11201.0   \n",
       "73   173694     W_Completeren aanvraag-START   11201.0   \n",
       "74   173694              A_ACCEPTED-COMPLETE   11201.0   \n",
       "75   173694             A_FINALIZED-COMPLETE   11201.0   \n",
       "76   173694              O_SELECTED-COMPLETE   11201.0   \n",
       "77   173694               O_CREATED-COMPLETE   11201.0   \n",
       "78   173694                  O_SENT-COMPLETE   11201.0   \n",
       "79   173694     W_Nabellen offertes-SCHEDULE   11201.0   \n",
       "80   173694  W_Completeren aanvraag-COMPLETE   11201.0   \n",
       "81   173694        W_Nabellen offertes-START   11201.0   \n",
       "82   173694              O_SELECTED-COMPLETE   11201.0   \n",
       "\n",
       "         Complete Timestamp      Variant  Variant index  (case) AMOUNT_REQ  \\\n",
       "63  2011/10/01 15:10:30.287  Variant 615            615               7000   \n",
       "64  2011/10/01 15:10:30.591  Variant 615            615               7000   \n",
       "65  2011/10/01 15:11:13.026  Variant 615            615               7000   \n",
       "66  2011/10/01 15:11:13.390  Variant 615            615               7000   \n",
       "67  2011/10/01 18:31:25.301  Variant 615            615               7000   \n",
       "68  2011/10/01 18:35:59.637  Variant 615            615               7000   \n",
       "69  2011/10/03 18:12:02.445  Variant 615            615               7000   \n",
       "70  2011/10/03 18:21:22.417  Variant 615            615               7000   \n",
       "71  2011/10/03 20:21:06.626  Variant 615            615               7000   \n",
       "72  2011/10/03 20:32:43.313  Variant 615            615               7000   \n",
       "73  2011/10/03 20:32:52.818  Variant 615            615               7000   \n",
       "74  2011/10/03 20:34:25.421  Variant 615            615               7000   \n",
       "75  2011/10/03 20:40:14.632  Variant 615            615               7000   \n",
       "76  2011/10/03 20:40:14.632  Variant 615            615               7000   \n",
       "77  2011/10/03 20:40:15.605  Variant 615            615               7000   \n",
       "78  2011/10/03 20:40:15.651  Variant 615            615               7000   \n",
       "79  2011/10/03 20:40:15.865  Variant 615            615               7000   \n",
       "80  2011/10/03 20:40:17.062  Variant 615            615               7000   \n",
       "81  2011/10/03 20:40:42.469  Variant 615            615               7000   \n",
       "82  2011/10/03 20:44:17.619  Variant 615            615               7000   \n",
       "\n",
       "    Accepted   duration  \n",
       "63         1   0.000000  \n",
       "64         1   0.000084  \n",
       "65         1   0.011787  \n",
       "66         1   0.000101  \n",
       "67         1   3.336642  \n",
       "68         1   0.076204  \n",
       "69         1  47.600780  \n",
       "70         1   0.155548  \n",
       "71         1   1.995614  \n",
       "72         1   0.193524  \n",
       "73         1   0.002640  \n",
       "74         1   0.025723  \n",
       "75         1   0.097003  \n",
       "76         1   0.000000  \n",
       "77         1   0.000270  \n",
       "78         1   0.000013  \n",
       "79         1   0.000059  \n",
       "80         1   0.000333  \n",
       "81         1   0.007058  \n",
       "82         1   0.059764  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQHcWkgznSry"
   },
   "source": [
    "# indexbase.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmIW86VanSrz",
    "outputId": "58cb7075-9397-4918-ea43-3ceb33c85f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix length : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                           | 197/9658 [00:00<00:04, 1950.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9658/9658 [00:04<00:00, 2081.16it/s]\n",
      "  2%|█▌                                                                           | 199/9658 [00:00<00:04, 1974.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9658/9658 [00:04<00:00, 1996.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "5 (case) AMOUNT_REQ \n",
      " 1/3 point :  7000.0 ,  2/3 point :  15000.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                          | 211/6981 [00:00<00:03, 2098.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6981/6981 [00:03<00:00, 2146.44it/s]\n",
      "  3%|██▎                                                                          | 210/6981 [00:00<00:03, 2090.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6981/6981 [00:03<00:00, 2017.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "10 (case) AMOUNT_REQ \n",
      " 1/3 point :  8000.0 ,  2/3 point :  16000.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                          | 208/6028 [00:00<00:02, 2067.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6028/6028 [00:02<00:00, 2132.74it/s]\n",
      "  7%|█████▎                                                                       | 418/6028 [00:00<00:02, 2084.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6028/6028 [00:02<00:00, 2091.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "15 (case) AMOUNT_REQ \n",
      " 1/3 point :  8000.0 ,  2/3 point :  16000.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                          | 177/5491 [00:00<00:03, 1755.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5491/5491 [00:02<00:00, 1857.78it/s]\n",
      "  7%|█████▋                                                                       | 404/5491 [00:00<00:02, 2010.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5491/5491 [00:02<00:00, 1958.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "20 (case) AMOUNT_REQ \n",
      " 1/3 point :  8000.0 ,  2/3 point :  16250.000000000073\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                         | 195/4568 [00:00<00:02, 1933.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4568/4568 [00:02<00:00, 1926.45it/s]\n",
      "  4%|███▎                                                                         | 195/4568 [00:00<00:02, 1931.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4568/4568 [00:02<00:00, 2000.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "25 (case) AMOUNT_REQ \n",
      " 1/3 point :  8000.0 ,  2/3 point :  17000.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▎                                                                        | 198/3532 [00:00<00:01, 1962.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3532/3532 [00:01<00:00, 2021.82it/s]\n",
      " 12%|████████▉                                                                    | 408/3532 [00:00<00:01, 2034.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3532/3532 [00:01<00:00, 2052.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "30 (case) AMOUNT_REQ \n",
      " 1/3 point :  8500.0 ,  2/3 point :  17500.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▊                                                                       | 202/2659 [00:00<00:01, 2009.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2659/2659 [00:01<00:00, 2042.49it/s]\n",
      " 13%|█████████▊                                                                   | 340/2659 [00:00<00:01, 1684.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2659/2659 [00:01<00:00, 1848.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "35 (case) AMOUNT_REQ \n",
      " 1/3 point :  9000.0 ,  2/3 point :  18000.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n",
      "Prefix length : 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                     | 196/1948 [00:00<00:00, 1943.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start event categorical attribute OHE preprocessing.... \n",
      "\n",
      "Activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1948/1948 [00:00<00:00, 1976.37it/s]\n",
      "  9%|██████▊                                                                      | 172/1948 [00:00<00:01, 1718.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1948/1948 [00:01<00:00, 1932.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Case continuous attribute OHE preprocessing.... \n",
      "\n",
      "(case) AMOUNT_REQ\n",
      "40 (case) AMOUNT_REQ \n",
      " 1/3 point :  9500.000000000011 ,  2/3 point :  18000.0\n",
      "Start y value extraction preprocessing.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import os \n",
    "import multiprocessing\n",
    "from multiprocessing import Pool,cpu_count\n",
    "#from playsound import playsound\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def indexbase(df,prefix): #Activity index base encoding ex) e1_a, e1_b, e2_d,e2_f...\n",
    "    print('Start index base preprocessing.... \\n')\n",
    "    groups = df.groupby('Case ID')\n",
    "    prefixlength =  prefix\n",
    "    obj = {}\n",
    "    case_name =[]\n",
    "\n",
    "    for i in range(prefixlength):\n",
    "        name = 'event'+str(i+1)\n",
    "        obj[name]=[]\n",
    "\n",
    "    length2 = len(set(df['Case ID']))\n",
    "    for case,group in tqdm(groups):\n",
    "        group = group.sort_values('Complete Timestamp').reset_index(drop=True)\n",
    "        actlist = list(group['Activity'])\n",
    "        case_name.append(case)\n",
    "        for pos,act in enumerate(actlist):\n",
    "            name = 'event'+str(pos+1)\n",
    "            obj[name].append(act)\n",
    "    dfk = pd.DataFrame(obj)\n",
    "    columns = list(dfk.columns)\n",
    "    concating=[]\n",
    "    for col in columns:\n",
    "        concating.append(pd.get_dummies(dfk[col],prefix=col))\n",
    "    dfk = pd.concat(concating,axis=1)\n",
    "    dfk['Case ID'] = case_name\n",
    "    return dfk\n",
    "\n",
    "def case_att_cat_onehot(df,columns): #Case attribute categorical column one hot encoding\n",
    "    '''\n",
    "    columns : list type object\n",
    "    '''\n",
    "\n",
    "    print('Start Case categorical attribute OHE preprocessing.... \\n')\n",
    "    groups = df.groupby('Case ID').first()\n",
    "    concating =[]\n",
    "    case =[]\n",
    "    for col in columns:\n",
    "        concating.append(pd.get_dummies(groups[col],prefix=col.capitalize()))\n",
    "    dfk =pd.concat(concating,axis=1).reset_index(drop=True)\n",
    "    dfk['Case ID'] = list(groups.index.values)\n",
    "    return dfk\n",
    "\n",
    "def case_att_con_onehot(df,columns): #Case attribute categorical column one hot encoding\n",
    "    '''\n",
    "    columns : list type object\n",
    "    '''\n",
    "    \n",
    "    print('Start Case continuous attribute OHE preprocessing.... \\n')\n",
    "    groups = df.groupby('Case ID').first()\n",
    "    concating=[]\n",
    "    obj ={}\n",
    "    for col in columns:\n",
    "        if df[col].isnull().sum() == len(df):\n",
    "            pass\n",
    "        else:\n",
    "            print(col)\n",
    "            toput = []\n",
    "            for x in list(groups[col]):\n",
    "                if math.isnan(x):\n",
    "                    pass\n",
    "                else:\n",
    "                    toput.append(x)\n",
    "            obj[col] = toput\n",
    "            \n",
    "\n",
    "            shortpoint = np.percentile(obj[col],100/3)\n",
    "            midpoint = np.percentile(obj[col],200/3)\n",
    "\n",
    "            obj2={}\n",
    "\n",
    "            length2 = len(set(df['Case ID']))\n",
    "\n",
    "            print(prefix, col, '\\n', '1/3 point : ',shortpoint, ',  2/3 point : ',midpoint)\n",
    "\n",
    "            categorizedlist=[]\n",
    "            targetcol = list(groups[col])\n",
    "            for target in targetcol:\n",
    "                if target < shortpoint:\n",
    "                    categorizedlist.append('L-'+str(shortpoint))\n",
    "                elif target >= shortpoint and target < midpoint:\n",
    "                    categorizedlist.append('L-'+str(shortpoint)+'U-'+str(midpoint))\n",
    "                elif target >= midpoint:\n",
    "                    categorizedlist.append('U-'+str(midpoint))\n",
    "                else:\n",
    "                    categorizedlist.append('Nan')\n",
    "\n",
    "            groups[col] = categorizedlist\n",
    "            dfk = pd.get_dummies(groups[col],prefix=col)\n",
    "            if len(columns) ==1:\n",
    "                pass\n",
    "            else:\n",
    "                concating.append(dfk)\n",
    "        if len(columns) ==1:\n",
    "            pass\n",
    "        else:    \n",
    "            dfk = pd.concat(concating,axis=1)\n",
    "        # dfk['Case ID'] = list(df.groupby('Case ID').first().index.values)\n",
    "    return dfk\n",
    "    \n",
    "def event_att_cat_onehot(df,columns,prefix):\n",
    "    '''\n",
    "    columns : list type object\n",
    "    prefix : prefix length\n",
    "    '''\n",
    "    print('Start event categorical attribute OHE preprocessing.... \\n')\n",
    "    groups = df.groupby('Case ID')\n",
    "    prefixlength =  prefix\n",
    "\n",
    "    dropcol=[]\n",
    "    for x in columns:\n",
    "        if df[x].isnull().all():\n",
    "            dropcol.append(x)\n",
    "    columns = [x for x in columns if x not in dropcol]\n",
    "\n",
    "    allconcating=[]\n",
    "    for col in columns:\n",
    "        print(col)\n",
    "        obj = {}\n",
    "        case_name =[]\n",
    "        for i in range(prefixlength):\n",
    "            name = str(col)+str(i+1)\n",
    "            obj[name]=[]\n",
    "\n",
    "        length2 = len(set(df['Case ID']))\n",
    "       \n",
    "        for case,group in tqdm(groups):\n",
    "            group = group.sort_values('Complete Timestamp').reset_index(drop=True)\n",
    "            actlist = list(group[col])\n",
    "            case_name.append(case)\n",
    "            for pos,act in enumerate(actlist):\n",
    "                name = str(col)+str(pos+1)\n",
    "                obj[name].append(act)\n",
    "\n",
    "        dfk = pd.DataFrame(obj)\n",
    "        columns = list(dfk.columns)\n",
    "        concating=[]\n",
    "        for col in columns:\n",
    "            concating.append(pd.get_dummies(dfk[col],prefix=col))\n",
    "        dfk = pd.concat(concating,axis=1)\n",
    "\n",
    "        if len(columns) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            allconcating.append(dfk)\n",
    "    if len(columns) ==1:\n",
    "        pass\n",
    "    else:\n",
    "        dfk = pd.concat(allconcating,axis=1)\n",
    "    \n",
    "    dfk['Case ID'] = case_name\n",
    "    return dfk\n",
    "\n",
    "\n",
    "def event_att_con_onehot(df,columns,prefix): #Case attribute categorical column one hot encoding\n",
    "    '''\n",
    "    columns : list type object\n",
    "    prefix : prefix length\n",
    "    '''\n",
    "\n",
    "    print('Start Event continuous attribute OHE preprocessing.... \\n')\n",
    "    groups = df.groupby('Case ID')\n",
    "    concating =[]\n",
    "    \n",
    "\n",
    "    prefixlength =  prefix\n",
    "\n",
    "    dropcol=[]\n",
    "    for x in columns:\n",
    "        if df[x].isnull().all():\n",
    "            dropcol.append(x)\n",
    "    columns = [x for x in columns if x not in dropcol]\n",
    "\n",
    "    for col in columns:\n",
    "        print(col)\n",
    "        obj = {}\n",
    "        for case, group in groups:\n",
    "            event_attribute = list(group[col])\n",
    "            for pos, att in enumerate(event_attribute):\n",
    "                name = str(col).capitalize()+str(pos+1)\n",
    "                if name not in list(obj.keys()):\n",
    "                    obj[name] = [[att],1,{},[]]\n",
    "                else:                \n",
    "                    obj[name][0].append(att)\n",
    "                    obj[name][1] +=1\n",
    "        objkey = list(obj.keys())\n",
    "\n",
    "        for key in obj.keys():\n",
    "            obj[key][0] = [x for x in obj[key][0] if ~np.isnan(x)]\n",
    "            obj[key][2]['Shortpoint'] = np.mean(obj[key][0])*0.4\n",
    "            obj[key][2]['Midpoint'] = np.mean(obj[key][0])*0.6\n",
    "\n",
    "                \n",
    "        for case, group in groups:\n",
    "            event_attribute = list(group[col])\n",
    "            for pos, att in enumerate(event_attribute):\n",
    "                name = str(col).capitalize()+str(pos+1)\n",
    "                if name not in list(obj.keys()):\n",
    "                    pass\n",
    "                else:\n",
    "                    shortpoint = obj[name][2]['Shortpoint']\n",
    "                    midpoint  = obj[name][2]['Midpoint']\n",
    "                    if att < shortpoint:\n",
    "                        obj[name][3].append('L-'+str(shortpoint))\n",
    "                    elif att >= shortpoint and att < midpoint:\n",
    "                        obj[name][3].append('L-'+str(shortpoint)+'U-'+str(midpoint))\n",
    "                    elif att > midpoint:\n",
    "                        obj[name][3].append('U-'+str(midpoint))\n",
    "                    else:\n",
    "                        obj[name][3].append('Nan')\n",
    "\n",
    "        obj2 ={x:obj[x][3] for x in obj.keys()}\n",
    "\n",
    "        dfk = pd.DataFrame(obj2)\n",
    "        columns2 = list(dfk.columns)\n",
    "        concating2=[]\n",
    "        for col in columns2:\n",
    "            concating2.append(pd.get_dummies(dfk[col],prefix=col))\n",
    "        dfk = pd.concat(concating2,axis=1)\n",
    "        concating.append(dfk)\n",
    "        print(prefix, col, '\\n', '1/3 point : ', shortpoint, ',  2/3 point : ',midpoint)\n",
    "    \n",
    "    \n",
    "    dft = pd.concat(concating,axis=1)\n",
    "    dft = dft.reset_index(drop=True)\n",
    "    dft['Case ID'] = list(df.groupby('Case ID').first().index.values)\n",
    "    \n",
    "    return dft\n",
    "\n",
    "\n",
    "def y_value(df,column):\n",
    "    print('Start y value extraction preprocessing.... \\n')\n",
    "    m_dict={'Case ID':[],column:[]}\n",
    "    df = df.rename(columns={column:'Label'})\n",
    "    \n",
    "    column = 'Label'\n",
    "    groups = df.groupby('Case ID').first()\n",
    "    \n",
    "    m_dict['Case ID']=list(groups.index.values)\n",
    "    m_dict[column] = list(groups[column])\n",
    "    dfk = pd.DataFrame(m_dict,columns=['Case ID','Label'])\n",
    "    \n",
    "    dfk = pd.concat([dfk,pd.get_dummies(dfk['Label'],prefix=column)],axis=1)\n",
    "    dfk = dfk.drop(column,axis=1)\n",
    "    return dfk\n",
    "\n",
    "def timediscretize(df,prefix): #3 cat, [short,medium,long] discretize\n",
    "    print('Start time discretization preprocessing.... \\n')\n",
    "    df['Complete Timestamp'] =  pd.to_datetime(df['Complete Timestamp'])\n",
    "    timeorder = {}\n",
    "    groups = df.groupby('Case ID')\n",
    "\n",
    "    length2 = len(set(df['Case ID']))\n",
    "\n",
    "    for case,group in tqdm(groups):\n",
    "        group = group.sort_values('Complete Timestamp').reset_index(drop=True)\n",
    "        actlist = list(group['Activity'])\n",
    "        length = len(group)\n",
    "        timestamp =list(group['Complete Timestamp'])\n",
    "        for pos, x in enumerate(actlist):\n",
    "            if pos+1 != length:\n",
    "                if ((x,actlist[pos+1])) not in list(timeorder.keys()):\n",
    "                    timeorder[(x,actlist[pos+1])] =[[(timestamp[pos+1]- timestamp[pos]).total_seconds() / 3600],1,{}]\n",
    "                else:\n",
    "                    timeorder[(x,actlist[pos+1])][0].append((timestamp[pos+1]- timestamp[pos]).total_seconds() / 3600)\n",
    "                    timeorder[(x,actlist[pos+1])][1] +=1\n",
    "    \n",
    "    duration = [0]\n",
    "    for j in range(len(df)-1):\n",
    "\n",
    "        if df.iloc[j]['Case ID'] == df.iloc[j+1]['Case ID']:\n",
    "            x = pd.to_datetime(df.iloc[j]['Complete Timestamp']).timestamp()\n",
    "            y = pd.to_datetime(df.iloc[j+1]['Complete Timestamp']).timestamp()\n",
    "            duration.append((y-x)/3600)\n",
    "        else:\n",
    "            duration.append(0)\n",
    "\n",
    "    non_zero  = []\n",
    "    for j in range(len(duration)):\n",
    "        if duration[j] != 0:\n",
    "            non_zero.append(duration[j])\n",
    "\n",
    "    shortpoint = np.percentile(non_zero,100/3)\n",
    "    midpoint = np.percentile(non_zero,200/3)\n",
    "\n",
    "    print(prefix, shortpoint, midpoint)\n",
    "\n",
    "\n",
    "    df['Complete Timestamp'] =  pd.to_datetime(df['Complete Timestamp'])\n",
    "    prefixlength =  prefix\n",
    "    obj = {}\n",
    "    case_name =[]\n",
    "    for i in range(prefixlength-1):\n",
    "        name = 'Time'+str(i+1)\n",
    "        obj[name]=[]\n",
    "\n",
    "    length2 = len(set(df['Case ID']))\n",
    "\n",
    "    for case,group in groups:\n",
    "        group = group.sort_values('Complete Timestamp').reset_index(drop=True)\n",
    "        actlist = list(group['Activity'])\n",
    "        case_name.append(case)\n",
    "        timestamp =list(group['Complete Timestamp'])\n",
    "        for pos,x in enumerate(actlist):\n",
    "            if pos+1 != length:\n",
    "                actpair = ((x,actlist[pos+1]))\n",
    "                timedifference = (timestamp[pos] - timestamp[pos-1]).total_seconds() /3600\n",
    "                name = 'Time'+str(pos+1)\n",
    "                if timedifference < shortpoint:\n",
    "                    obj[name].append('L-'+str(shortpoint))\n",
    "                elif timedifference >= shortpoint and timedifference < midpoint:\n",
    "                    obj[name].append('L-'+str(shortpoint)+'U-'+str(midpoint))\n",
    "                elif timedifference >= midpoint:\n",
    "                    obj[name].append('U-'+str(midpoint))\n",
    "                else:\n",
    "                    obj[name].append('all0')\n",
    "            \n",
    "    dfk = pd.DataFrame(obj)\n",
    "    columns = list(dfk.columns)\n",
    "    concating=[]\n",
    "    for col in columns:\n",
    "        concating.append(pd.get_dummies(dfk[col],prefix=col))\n",
    "    dfk = pd.concat(concating,axis=1)\n",
    "    dfk['Case ID'] = case_name    \n",
    "    \n",
    "\n",
    "    return dfk\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    for k in range(5, 41, 5):\n",
    "        prefix = k\n",
    "        print('Prefix length : %s'%(prefix))\n",
    "\n",
    "        dirname = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\prefix'+str(prefix)\n",
    "        df = pd.read_csv(dirname+'/bpic2012_prep.csv')\n",
    "\n",
    "        \n",
    "        case_att_con = ['(case) AMOUNT_REQ']\n",
    "        #case_att_cat = ['Selected', 'Accepted']\n",
    "        event_att_cat = ['Activity', 'Resource']\n",
    "        #event_att_con = ['duration']\n",
    "\n",
    "            \n",
    "        savedir = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\RIPPER\\prefix'+str(prefix)+''\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(savedir)\n",
    "        except:\n",
    "            pass        \n",
    "        \n",
    "        y_column = 'Accepted'\n",
    "\n",
    "        event = {}\n",
    "        \n",
    "        for p in range(prefix):\n",
    "            dict_index = 'duration{}'.format(p+1)\n",
    "            event[dict_index] = []\n",
    "            for case, group in df.groupby('Case ID'):\n",
    "                dur_list = group['duration'].tolist()\n",
    "                event[dict_index].append(dur_list[p])\n",
    "        \n",
    "        groups = df.groupby('Case ID').first()\n",
    "        event['Case ID'] = list(groups.index.values)\n",
    "#         event['Accepted'] = list(groups.Accepted.values)\n",
    "        event['Complete Timestamp'] = df.groupby('Case ID').tail(1)['Complete Timestamp']\n",
    "\n",
    "        dur_df = pd.DataFrame.from_dict(event)\n",
    "        \n",
    "        dfs = [dur_df,event_att_cat_onehot(df,event_att_cat,prefix),\n",
    "        case_att_con_onehot(df,case_att_con), y_value(df,y_column)]\n",
    "\n",
    "        \n",
    "        df_final = reduce(lambda  left,right: pd.merge(left,right,on='Case ID'),dfs)\n",
    "\n",
    "\n",
    "        df_final.to_csv(savedir +'/BPICinput_preprocessed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for k in range(5, 41, 5):\n",
    "    prefix = k\n",
    "    savedir = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\RIPPER\\prefix'+str(prefix)+''\n",
    "    df_final=pd.read_csv(savedir +'/BPICinput_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_final[df_final['Label_1']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkIFIuoJnSr6"
   },
   "source": [
    "# trainsplit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD7aSP7dnSr8",
    "outputId": "d94b7c41-d9c6-4ad0-bdb9-8d38a1df8db6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:42<00:00,  5.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "for prefix in tqdm(range(5, 41, 5)):\n",
    "\n",
    "    dir_path = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\RIPPER\\prefix'+str(prefix)+''\n",
    "    df = pd.read_csv(dir_path+'/BPICinput_preprocessed.csv')\n",
    "    df['Complete Timestamp'] =  pd.to_datetime(df['Complete Timestamp'])\n",
    "    df = df.sort_values('Complete Timestamp').reset_index(drop=True)\n",
    "    \n",
    "    for rndst in range(0,5):\n",
    "        df_train,df_test = train_test_split(df,test_size=0.3, shuffle=False, random_state=rndst) #Random State 0,1,2,3,4,5,6,7,8,9 10 numbers\n",
    "        df_train.to_csv(dir_path+'/train_rndst'+str(rndst)+'.csv',index=False)\n",
    "        df_test.to_csv(dir_path+'/test_rndst'+str(rndst)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Wo54oWOnSr9",
    "outputId": "c4966ed5-6030-4a1d-ecfd-e7c147729ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363, 2770) (585, 2770)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf3MGmPZoNRf"
   },
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Zirk5OdnoQmm",
    "outputId": "6d6cb930-d069-4a74-e7c7-a92b5240fdb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.7891161680321461, 0.0001323370474647953], 'recall': [0.9811702595688516, 0.0018748152883997075], 'f1-score': [0.8747246658440776, 0.0007446059790302659], 'support': [2273.0, 0.0]}, 'Label 1': {'precision': [0.4048603604488905, 0.013409422152985222], 'recall': [0.046400000000000004, 0.0020238577025077625], 'f1-score': [0.08321572230902992, 0.002999331515233576], 'support': [625.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.7523688887550719, 0.0008111480394802402], 'recall': [0.8371752165223183, 0.005066796360248251], 'f1-score': [0.7925006663899856, 0.0020273898667522083], 'support': [1501.0, 0.0]}, 'Label 1': {'precision': [0.42473525174154725, 0.003358268979372258], 'recall': [0.3037037037037037, 0.006528861760830072], 'f1-score': [0.35410408116296355, 0.003915077764630744], 'support': [594.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.725786800430037, 0.0033747027830102576], 'recall': [0.7922824302134648, 0.004926108374384238], 'f1-score': [0.7575726649110689, 0.0035852461584660053], 'support': [1218.0, 0.0]}, 'Label 1': {'precision': [0.4722665623508335, 0.00920808431029443], 'recall': [0.3830795262267343, 0.009055220413035291], 'f1-score': [0.42299965367649256, 0.00869059808004892], 'support': [591.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.7861039705191772, 0.005204414275149717], 'recall': [0.8148430066603234, 0.014833244734466256], 'f1-score': [0.8000927624803105, 0.004565479168732338], 'support': [1051.0, 0.0]}, 'Label 1': {'precision': [0.6520313355958023, 0.011021889114961117], 'recall': [0.6093802345058627, 0.019021868765934185], 'f1-score': [0.6296241313803039, 0.005086823898007307], 'support': [597.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.8196210105837007, 0.0022698069481346785], 'recall': [0.6955555555555556, 0.007513641508495298], 'f1-score': [0.7524837299692138, 0.0042911668359132595], 'support': [810.0, 0.0]}, 'Label 1': {'precision': [0.6393154806895268, 0.004951917448850339], 'recall': [0.7789661319073083, 0.004509486859420119], 'f1-score': [0.7022411494929781, 0.0025532235836126277], 'support': [561.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.8478490379225919, 0.002562377702707267], 'recall': [0.6604340567612688, 0.005744457607374048], 'f1-score': [0.7424796511282024, 0.0035038347238832497], 'support': [599.0, 0.0]}, 'Label 1': {'precision': [0.6572518784046725, 0.0033278491619372318], 'recall': [0.8459869848156183, 0.0036297615034016043], 'f1-score': [0.7397603249098406, 0.0020296983976159596], 'support': [461.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.8591832059353331, 0.002738952383566252], 'recall': [0.5985714285714286, 0.006834619092574928], 'f1-score': [0.7055652790597129, 0.005359499794692037], 'support': [420.0, 0.0]}, 'Label 1': {'precision': [0.6664232526749008, 0.003882875717503076], 'recall': [0.8910052910052911, 0.001979712903055016], 'f1-score': [0.762515825776181, 0.0028151688958920765], 'support': [378.0, 0.0]}}\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'Label 0': {'precision': [0.8567149401545728, 0.012201116584876107], 'recall': [0.5302816901408451, 0.01271511977835762], 'f1-score': [0.6550061490999839, 0.011484538705465874], 'support': [284.0, 0.0]}, 'Label 1': {'precision': [0.6740379224445535, 0.006544580627926819], 'recall': [0.9162790697674419, 0.0076915859819204355], 'f1-score': [0.7766910053669969, 0.006042971214436739], 'support': [301.0, 0.0]}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from xgboost import XGBClassifier,plot_importance\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def cuttinginput(df,alpha):\n",
    "    try:\n",
    "        df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_cols =df.columns.values\n",
    "    df_collist=['Case ID','Label_1','Label_0']\n",
    "    for k in df_cols:\n",
    "        if round(len(df[df[k]==1])/len(df),2) >alpha:\n",
    "            if k not in df_collist:\n",
    "                df_collist.append(k)\n",
    "\n",
    "    df = df.loc[:,df_collist]\n",
    "    return df, df_collist\n",
    "\n",
    "def rfclassifer():\n",
    "    alpha = 0\n",
    "    for prefixlength in range(5, 41, 5):\n",
    "        resultdict={}\n",
    "        resultdict['Label 0'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        resultdict['Label 1'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        for rndst in range(0,5):\n",
    "            \n",
    "            \n",
    "            wholefile = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\RIPPER\\prefix'+str(prefixlength)+'\\BPICinput_preprocessed.csv'\n",
    "            train = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\RIPPER\\prefix'+str(prefixlength)+'/train_rndst'+str(rndst)+'.csv'\n",
    "            test = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\RIPPER\\prefix'+str(prefixlength)+'/test_rndst'+str(rndst)+'.csv'\n",
    "            \n",
    "            wholefile,wh_collist = cuttinginput(pd.read_csv(wholefile),alpha)\n",
    "\n",
    "            \n",
    "            label = []\n",
    "            train = pd.read_csv(train)\n",
    "            test = pd.read_csv(test)\n",
    "            try:\n",
    "                train = train.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                test = test.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            train = train.loc[:,wh_collist]\n",
    "            test = test.loc[:,wh_collist]\n",
    "\n",
    "            for k in list(train['Label_1']):\n",
    "                if k==1:\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "            y_train = label\n",
    "            train = train.drop(['Label_1','Label_0','Case ID'],axis=1)\n",
    "            try:\n",
    "                test = test.drop(['Unnamed: 0'],axis=1)\n",
    "            except:\n",
    "                pass\n",
    "            x_train = train\n",
    "\n",
    "            label = []\n",
    "            for k in list(test['Label_1']):\n",
    "                if k==1:\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "\n",
    "            y_test = label\n",
    "            test = test.drop(['Label_1','Label_0','Case ID'],axis=1)\n",
    "            x_test = test\n",
    "            n_estimators = [100, 200, 300] # number of trees in the random forest\n",
    "            max_features = ['auto', 'sqrt'] # number of features in consideration at every split\n",
    "            max_depth = [int(x) for x in np.linspace(10, 1001, num = 110)] # maximum number of levels allowed in each decision tree\n",
    "            min_samples_split = [2, 6, 10] # minimum sample number to split a node\n",
    "            min_samples_leaf = [1, 4, 7] # minimum sample number that can be stored in a leaf node\n",
    "            bootstrap = [True, False] # method used to sample data points\n",
    "            random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            randmrf = RandomForestClassifier()\n",
    "            from sklearn.model_selection import RandomizedSearchCV\n",
    "            rf_random = RandomizedSearchCV(estimator = randmrf,param_distributions = random_grid,\n",
    "               n_iter = 25, cv = 5, verbose=2, random_state=35, n_jobs = -1)\n",
    "            rf_random.fit(x_train, y_train)\n",
    "            \n",
    "            rf=rf_random.best_estimator_\n",
    "            rf_pred = rf.predict(x_test)\n",
    "            result =classification_report(y_test,rf_pred,target_names=['Label 0','Label 1'],output_dict=True)\n",
    "            resultdict['Label 0']['precision'].append(result['Label 0']['precision'])\n",
    "            resultdict['Label 0']['recall'].append(result['Label 0']['recall'])\n",
    "            resultdict['Label 0']['f1-score'].append(result['Label 0']['f1-score'])\n",
    "            resultdict['Label 0']['support'].append(result['Label 0']['support'])\n",
    "            resultdict['Label 1']['precision'].append(result['Label 1']['precision'])\n",
    "            resultdict['Label 1']['recall'].append(result['Label 1']['recall'])\n",
    "            resultdict['Label 1']['f1-score'].append(result['Label 1']['f1-score'])\n",
    "            resultdict['Label 1']['support'].append(result['Label 1']['support'])\n",
    "\n",
    "        for pre in resultdict.keys():\n",
    "            for col in resultdict[pre].keys():\n",
    "                resultdict[pre][col] = [np.mean(resultdict[pre][col]),np.std(resultdict[pre][col])]\n",
    "\n",
    "\n",
    "        resultdir = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\prefix\\ruleresult\\randomforest'\n",
    "        try:    \n",
    "            os.makedirs(resultdir)\n",
    "        except:\n",
    "            pass\n",
    "        jsonname = resultdir+'/prefix'+str(prefixlength)+'result.json'\n",
    "        print(resultdict)\n",
    "        with open(jsonname ,'w') as f:\n",
    "            json.dump(resultdict,f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rfreader():\n",
    "    alpha = 0\n",
    "    for prefixlength in range(5, 41, 5):\n",
    "        # print(prefixlength)\n",
    "        resultdir = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\prefix\\withoutsparse_'+str(alpha)+'/'\n",
    "        jsonname = resultdir+'/prefix'+str(prefixlength)+'result.json'\n",
    "        \n",
    "        with open(jsonname ,'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(data['Label 1']['f1-score'][0])\n",
    "        # print(data['Label 1']['f1-score'][0])\n",
    "rfclassifer()\n",
    "# rfreader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiADROEdolFP"
   },
   "source": [
    "# xgb_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.1-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\dias\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dias\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix :5 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[14:53:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :10 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:32:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :15 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[16:26:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :20 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[17:06:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :25 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[17:37:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :30 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[18:03:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :35 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[18:21:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prefix :40 Rndst :0\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[18:36:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier,plot_importance\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "# dataset = pd.read_csv('./bpic2015/ltl1/bpic2015_1/indexbase/prefix5/simple_timediscretize/ARMinput_preprocessed.csv')\n",
    "# label = []\n",
    "# for k in list(dataset['Label_1']):\n",
    "#     if k==1:\n",
    "#         label.append(1)\n",
    "#     else:\n",
    "#         label.append(0)\n",
    "# y = label\n",
    "# dataset = dataset.drop(['Label_1','Label_0','Case ID'],axis=1)\n",
    "\n",
    "# x_train,x_test,y_train,y_test = train_test_split(dataset,y,test_size=0.3,random_state = 0)\n",
    "\n",
    "\n",
    "# xgb = XGBClassifier(n_estimators = 500,learning_rate=0.1,max_depth=4)\n",
    "# xgb.fit(x_train,y_train)\n",
    "# xgb_pred = xgb.predict(x_test)\n",
    "# print(classification_report(y_test,xgb_pred))\n",
    "\n",
    "# fig,ax = plt.subplots()\n",
    "# plot_importance(xgb,ax=ax)\n",
    "# plt.show()\n",
    "\n",
    "def cuttinginput(df,alpha):\n",
    "    try:\n",
    "        df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_cols =df.columns.values\n",
    "    df_collist=['Case ID','Label_1','Label_0']\n",
    "    for k in df_cols:\n",
    "        if round(len(df[df[k]==1])/len(df),2) >alpha:\n",
    "            if k not in df_collist:\n",
    "                df_collist.append(k)\n",
    "\n",
    "    df = df.loc[:,df_collist]\n",
    "    return df, df_collist\n",
    "\n",
    "\n",
    "\n",
    "def xgboosting():\n",
    "    for prefixlength in range(5, 41, 5):\n",
    "        resultdict={}\n",
    "        resultdict['Label 0'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        resultdict['Label 1'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        for rndst in range(0, 1):\n",
    "            print('Prefix :%s Rndst :%s'%(prefixlength,rndst))\n",
    "            alpha = 0\n",
    "            wholefile = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/RIPPER/prefix'+str(prefixlength)+'/BPICinput_preprocessed.csv'\n",
    "            wholefile = pd.read_csv(wholefile)\n",
    "\n",
    "            wholefile,wh_collist = cuttinginput(wholefile,alpha)\n",
    "           \n",
    "            train ,test = train_test_split(wholefile,test_size=0.3)\n",
    "            label = []\n",
    "            try:\n",
    "                train = train.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                test = test.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            for k in list(train['Label_1']):\n",
    "                if k==1:\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "            y_train = label\n",
    "            train = train.drop(['Label_1','Label_0','Case ID'],axis=1)\n",
    "            try:\n",
    "                test = test.drop(['Unnamed: 0'],axis=1)\n",
    "            except:\n",
    "                pass\n",
    "            x_train = train\n",
    "\n",
    "            label = []\n",
    "            for k in list(test['Label_1']):\n",
    "                if k==1:\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "\n",
    "            y_test = label\n",
    "            test = test.drop(['Label_1','Label_0','Case ID'],axis=1)\n",
    "            x_test = test\n",
    "            \n",
    "            n_estimators = [100, 200, 300] # number of trees in the random forest\n",
    "            learning_rate=[0.1, 0.3, 0.5]\n",
    "            subsample=[0.3, 0.5, 0.7]\n",
    "            max_depth = [int(x) for x in np.linspace(10, 1001, num = 110)] # maximum number of levels allowed in each decision tree\n",
    "            colsample_bytree=[0.5, 0.6, 0.7]\n",
    "            min_child_weight=[1, 2, 3, 4]\n",
    "            random_grid = {'n_estimators': n_estimators, 'learning_rate':learning_rate, 'subsample':subsample, \n",
    "                           'max_depth': max_depth, 'colsample_bytree':  colsample_bytree, 'min_child_weight':min_child_weight}\n",
    "            \n",
    "            randxgb = XGBClassifier()\n",
    "            from sklearn.model_selection import RandomizedSearchCV\n",
    "            xgb_random = RandomizedSearchCV(estimator = randxgb, param_distributions = random_grid, cv=5, n_iter = 25, \n",
    "                                                              scoring = 'roc_auc', error_score = 0, verbose = 3, n_jobs = -1)\n",
    "            xgb_random.fit(x_train, y_train)\n",
    "\n",
    "            xgb=xgb_random.best_estimator_\n",
    "            xgb_pred = xgb.predict(x_test)\n",
    "            result =classification_report(y_test,xgb_pred,target_names=['Label 0','Label 1'],output_dict=True)\n",
    "            resultdict['Label 0']['precision'].append(result['Label 0']['precision'])\n",
    "            resultdict['Label 0']['recall'].append(result['Label 0']['recall'])\n",
    "            resultdict['Label 0']['f1-score'].append(result['Label 0']['f1-score'])\n",
    "            resultdict['Label 0']['support'].append(result['Label 0']['support'])\n",
    "            resultdict['Label 1']['precision'].append(result['Label 1']['precision'])\n",
    "            resultdict['Label 1']['recall'].append(result['Label 1']['recall'])\n",
    "            resultdict['Label 1']['f1-score'].append(result['Label 1']['f1-score'])\n",
    "            resultdict['Label 1']['support'].append(result['Label 1']['support'])\n",
    "\n",
    "        for pre in resultdict.keys():\n",
    "            for col in resultdict[pre].keys():\n",
    "                resultdict[pre][col] = [np.mean(resultdict[pre][col]),np.std(resultdict[pre][col])]\n",
    "                \n",
    "        resultdir = r'C:\\Users\\Dias\\Desktop\\data\\dataset\\bpic2012\\prefix\\ruleresult\\xgboost'\n",
    "        try:\n",
    "            os.makedirs(resultdir)\n",
    "        except:\n",
    "            pass\n",
    "        jsonname = resultdir+'/prefix'+str(prefixlength)+'result.json'\n",
    "        with open(jsonname ,'w') as f:\n",
    "            json.dump(resultdict,f)\n",
    "\n",
    "'''\n",
    "for prefixlength in range(2,10):\n",
    "    resultdict={}\n",
    "    resultdict['Label 0'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "    resultdict['Label 1'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "\n",
    "    alpha = 0.2\n",
    "    wholefile = './road traffic/rule1/indexbase/prefix'+str(prefixlength)+'/simple_timediscretize/ARMinput_preprocessed.csv'\n",
    "    wholefile = pd.read_csv(wholefile)\n",
    "\n",
    "    wholefile,wh_collist = cuttinginput(wholefile,alpha)\n",
    "    print('Prefix :%s '%(prefixlength), len(wh_collist))\n",
    "'''\n",
    "xgboosting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix :5 Support :0.9 Rndst :0\n",
      "139\n",
      "8\n",
      "Prefix :10 Support :0.9 Rndst :0\n",
      "484\n",
      "8\n",
      "Prefix :15 Support :0.9 Rndst :0\n",
      "835\n",
      "8\n",
      "Prefix :20 Support :0.9 Rndst :0\n",
      "1259\n",
      "8\n",
      "Prefix :25 Support :0.9 Rndst :0\n",
      "1658\n",
      "8\n",
      "Prefix :30 Support :0.9 Rndst :0\n",
      "2065\n",
      "8\n",
      "Prefix :35 Support :0.9 Rndst :0\n",
      "2441\n",
      "8\n",
      "Prefix :40 Support :0.9 Rndst :0\n",
      "2770\n",
      "8\n",
      "Prefix :5 Support :0.9 Rndst :1\n",
      "139\n",
      "8\n",
      "Prefix :10 Support :0.9 Rndst :1\n",
      "484\n",
      "8\n",
      "Prefix :15 Support :0.9 Rndst :1\n",
      "835\n",
      "8\n",
      "Prefix :20 Support :0.9 Rndst :1\n",
      "1259\n",
      "8\n",
      "Prefix :25 Support :0.9 Rndst :1\n",
      "1658\n",
      "8\n",
      "Prefix :30 Support :0.9 Rndst :1\n",
      "2065\n",
      "8\n",
      "Prefix :35 Support :0.9 Rndst :1\n",
      "2441\n",
      "8\n",
      "Prefix :40 Support :0.9 Rndst :1\n",
      "2770\n",
      "8\n",
      "Prefix :5 Support :0.9 Rndst :2\n",
      "139\n",
      "8\n",
      "Prefix :10 Support :0.9 Rndst :2\n",
      "484\n",
      "8\n",
      "Prefix :15 Support :0.9 Rndst :2\n",
      "835\n",
      "8\n",
      "Prefix :20 Support :0.9 Rndst :2\n",
      "1259\n",
      "8\n",
      "Prefix :25 Support :0.9 Rndst :2\n",
      "1658\n",
      "8\n",
      "Prefix :30 Support :0.9 Rndst :2\n",
      "2065\n",
      "8\n",
      "Prefix :35 Support :0.9 Rndst :2\n",
      "2441\n",
      "8\n",
      "Prefix :40 Support :0.9 Rndst :2\n",
      "2770\n",
      "8\n",
      "Prefix :5 Support :0.9 Rndst :3\n",
      "139\n",
      "8\n",
      "Prefix :10 Support :0.9 Rndst :3\n",
      "484\n",
      "8\n",
      "Prefix :15 Support :0.9 Rndst :3\n",
      "835\n",
      "8\n",
      "Prefix :20 Support :0.9 Rndst :3\n",
      "1259\n",
      "8\n",
      "Prefix :25 Support :0.9 Rndst :3\n",
      "1658\n",
      "8\n",
      "Prefix :30 Support :0.9 Rndst :3\n",
      "2065\n",
      "8\n",
      "Prefix :35 Support :0.9 Rndst :3\n",
      "2441\n",
      "8\n",
      "Prefix :40 Support :0.9 Rndst :3\n",
      "2770\n",
      "8\n",
      "Prefix :5 Support :0.9 Rndst :4\n",
      "139\n",
      "8\n",
      "Prefix :10 Support :0.9 Rndst :4\n",
      "484\n",
      "8\n",
      "Prefix :15 Support :0.9 Rndst :4\n",
      "835\n",
      "8\n",
      "Prefix :20 Support :0.9 Rndst :4\n",
      "1259\n",
      "8\n",
      "Prefix :25 Support :0.9 Rndst :4\n",
      "1658\n",
      "8\n",
      "Prefix :30 Support :0.9 Rndst :4\n",
      "2065\n",
      "8\n",
      "Prefix :35 Support :0.9 Rndst :4\n",
      "2441\n",
      "8\n",
      "Prefix :40 Support :0.9 Rndst :4\n",
      "2770\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from mlxtend.frequent_patterns import fpgrowth,apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cuttinginput(df,alpha):\n",
    "    try:\n",
    "        df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "    except:\n",
    "        pass\n",
    "    print(len(df.columns.values))\n",
    "    df_cols =df.columns.values\n",
    "    df_collist=['Case ID','Label_1','Label_0']\n",
    "    for k in df_cols:\n",
    "        if round(len(df[df[k]==1])/len(df),2) >alpha:\n",
    "            if k not in df_collist:\n",
    "                df_collist.append(k)\n",
    "\n",
    "    df = df.loc[:,df_collist]\n",
    "    print(len(df_collist))\n",
    "    return df, df_collist\n",
    "\n",
    "\n",
    "\n",
    "def bpic2012():\n",
    "    for rndst in [0,1,2,3,4]:\n",
    "        for length in range(5, 41, 5):\n",
    "            for support in [0.9]:\n",
    "                alpha=0.7\n",
    "\n",
    "                print('Prefix :%s Support :%s Rndst :%s'%(length,support,rndst))       \n",
    "                dir_path = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/RIPPER/prefix'+str(length)\n",
    "                filename = dir_path + '/train_rndst'+str(rndst)+'.csv'\n",
    "                wholefile = dir_path+'/BPICinput_preprocessed.csv'\n",
    "                wholefile,wh_collist = cuttinginput(pd.read_csv(wholefile),alpha)\n",
    "\n",
    "                df = pd.read_csv(filename)\n",
    "                try:\n",
    "                    df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "                except:\n",
    "                    pass\n",
    "                threshold = 0.9\n",
    "                df =df.loc[:,wh_collist]\n",
    "                min_support = support\n",
    "                min_threshold = threshold\n",
    "                dir_path=dir_path+'/threshold'+str(threshold)+'/support_'+str(min_support)\n",
    "                try:\n",
    "                    os.makedirs(dir_path)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                df = df.drop(columns=['Case ID'],axis=1)\n",
    "\n",
    "                accepted = df[df['Label_1']==1]\n",
    "                refused = df[df['Label_0']==1]\n",
    "                '''\n",
    "                Accepted\n",
    "                '''\n",
    "                frequent_itemsets = apriori(accepted,min_support=min_support,use_colnames=True)\n",
    "                label1 = association_rules(frequent_itemsets,metric='confidence',min_threshold = min_threshold)\n",
    "                label1['consequents'] = [list(x) for x in list(label1['consequents'])]\n",
    "\n",
    "                labelin=[]\n",
    "                for pos,x in enumerate(list(label1['consequents'])):\n",
    "                    if 'Label_1' in x:\n",
    "                        labelin.append(pos)\n",
    "            \n",
    "                label1_name = dir_path+'/association_result_1_rnd'+str(rndst)+'.json'\n",
    "                label1 = label1.loc[labelin,:]#['antecedents','consequents']]\n",
    "                label1['antecedents'] = [list(x) for x in list(label1['antecedents'])] \n",
    "                label1.to_json(label1_name,orient='columns')\n",
    "                label1.to_csv(dir_path+'/label1result_rndst'+str(rndst)+'.csv',index=False)\n",
    "\n",
    "\n",
    "                \n",
    "                '''\n",
    "                Refused\n",
    "                '''\n",
    "                # print('Label 0!')\n",
    "                frequent_itemsets = apriori(refused,min_support=min_support,use_colnames=True)\n",
    "                label1 = association_rules(frequent_itemsets,metric='confidence',min_threshold = min_threshold)\n",
    "                label1['consequents'] = [list(x) for x in list(label1['consequents'])]\n",
    "\n",
    "                labelin=[]\n",
    "                for pos,x in enumerate(list(label1['consequents'])):\n",
    "                    if 'Label_0' in x:\n",
    "                        labelin.append(pos)\n",
    "\n",
    "                label1_name = dir_path+'/association_result_0_rnd'+str(rndst)+'.json'\n",
    "                label1 = label1.loc[labelin,:]#['antecedents','consequents']]\n",
    "                label1['antecedents'] = [list(x) for x in list(label1['antecedents'])]\n",
    "                label1.to_json(label1_name,orient='columns')\n",
    "                label1.to_csv(dir_path+'/label0result_rndst'+str(rndst)+'.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "bpic2012()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix : 5, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_5_label0_rnd_0.json\n",
      "Prefix : 10, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_10_label0_rnd_0.json\n",
      "Prefix : 15, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_15_label0_rnd_0.json\n",
      "Prefix : 20, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_20_label0_rnd_0.json\n",
      "Prefix : 25, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_25_label0_rnd_0.json\n",
      "Prefix : 30, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_30_label0_rnd_0.json\n",
      "Prefix : 35, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_35_label0_rnd_0.json\n",
      "Prefix : 40, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_40_label0_rnd_0.json\n",
      "Prefix : 5, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_5_label0_rnd_1.json\n",
      "Prefix : 10, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_10_label0_rnd_1.json\n",
      "Prefix : 15, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_15_label0_rnd_1.json\n",
      "Prefix : 20, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_20_label0_rnd_1.json\n",
      "Prefix : 25, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_25_label0_rnd_1.json\n",
      "Prefix : 30, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_30_label0_rnd_1.json\n",
      "Prefix : 35, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_35_label0_rnd_1.json\n",
      "Prefix : 40, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_40_label0_rnd_1.json\n",
      "Prefix : 5, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_5_label0_rnd_2.json\n",
      "Prefix : 10, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_10_label0_rnd_2.json\n",
      "Prefix : 15, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_15_label0_rnd_2.json\n",
      "Prefix : 20, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_20_label0_rnd_2.json\n",
      "Prefix : 25, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_25_label0_rnd_2.json\n",
      "Prefix : 30, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_30_label0_rnd_2.json\n",
      "Prefix : 35, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_35_label0_rnd_2.json\n",
      "Prefix : 40, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_40_label0_rnd_2.json\n",
      "Prefix : 5, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_5_label0_rnd_3.json\n",
      "Prefix : 10, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_10_label0_rnd_3.json\n",
      "Prefix : 15, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_15_label0_rnd_3.json\n",
      "Prefix : 20, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_20_label0_rnd_3.json\n",
      "Prefix : 25, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_25_label0_rnd_3.json\n",
      "Prefix : 30, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_30_label0_rnd_3.json\n",
      "Prefix : 35, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_35_label0_rnd_3.json\n",
      "Prefix : 40, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_40_label0_rnd_3.json\n",
      "Prefix : 5, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_5_label0_rnd_4.json\n",
      "Prefix : 10, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_10_label0_rnd_4.json\n",
      "Prefix : 15, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_15_label0_rnd_4.json\n",
      "Prefix : 20, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_20_label0_rnd_4.json\n",
      "Prefix : 25, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_25_label0_rnd_4.json\n",
      "Prefix : 30, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_30_label0_rnd_4.json\n",
      "Prefix : 35, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_35_label0_rnd_4.json\n",
      "Prefix : 40, Support :0.9\n",
      "C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold0.9/prefix_40_label0_rnd_4.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "#from playsound import playsound\n",
    "\n",
    "\n",
    "\n",
    "def way3():\n",
    "    for rndst in [0,1,2,3,4]:\n",
    "        for prefixlength in range(5,41, 5):\n",
    "            label1ruledict={}\n",
    "            label0ruledict={}\n",
    "            label0rule=set()\n",
    "            label1rule=set()\n",
    "            ruleset=set()\n",
    "            threshold=0.9\n",
    "            label1count = 0\n",
    "            label0count = 0\n",
    "            for support in [0.9]:\n",
    "                    print('Prefix : %s, Support :%s'%(prefixlength,support))\n",
    "                    dir_path = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/RIPPER/prefix'+str(prefixlength)\n",
    "                    dir_path=dir_path+'/threshold'+str(threshold)+'/support_'+str(support)\n",
    "                    label1 = dir_path+'/association_result_1_rnd'+str(rndst)+'.json'\n",
    "                    label0 = dir_path+'/association_result_0_rnd'+str(rndst)+'.json'\n",
    "                    label1ruledict[support] =set()\n",
    "                    label0ruledict[support] =set()\n",
    "\n",
    "                    with open(label1) as json_file:\n",
    "                        label1 = json.load(json_file)['antecedents']\n",
    "\n",
    "                    with open(label0) as json_file:\n",
    "                        label0 = json.load(json_file)['antecedents']\n",
    "\n",
    "                    for rule in label1.values():\n",
    "                        if len(rule) != 1:\n",
    "                            rule = sorted(rule)\n",
    "                            rule = '/'.join(rule)                               \n",
    "                        else:\n",
    "                            rule = rule[0]                        \n",
    "                        if rule not in label1rule:\n",
    "                            label1ruledict[support].add(rule)\n",
    "                            label1rule.add(rule)\n",
    "                        else:\n",
    "                            label1count +=1\n",
    "\n",
    "                    for rule in label0.values():\n",
    "                        if len(rule) !=1:\n",
    "                            rule = sorted(rule)\n",
    "                            rule = '/'.join(rule)                               \n",
    "                        else:\n",
    "                            rule = rule[0]\n",
    "                        \n",
    "                        if rule not in label0rule:\n",
    "                            label0ruledict[support].add(rule)\n",
    "                            label0rule.add(rule)\n",
    "                        else:\n",
    "                            label0count +=1\n",
    "            \n",
    "\n",
    "            for supp in label1ruledict.keys():\n",
    "                rules = list(label1ruledict[supp])\n",
    "                for value in rules:\n",
    "                    if value in label0rule:\n",
    "                        label1ruledict[supp].remove(value)\n",
    "\n",
    "            for supp in label0ruledict.keys():\n",
    "                rules = list(label0ruledict[supp])\n",
    "                for value in rules:\n",
    "                    if value in label1rule:\n",
    "                        label0ruledict[supp].remove(value)\n",
    "\n",
    "            for k in list(label1ruledict.keys()):\n",
    "                if len(label1ruledict[k])==0:\n",
    "                    del label1ruledict[k]\n",
    "                else:\n",
    "                    label1ruledict[k] = list(label1ruledict[k])\n",
    "\n",
    "            for k in list(label0ruledict.keys()):\n",
    "                if len(label0ruledict[k])==0:\n",
    "                    del label0ruledict[k]\n",
    "                else:\n",
    "                    label0ruledict[k] = list(label0ruledict[k])\n",
    "            \n",
    "            label0rulelist = []\n",
    "            label1rulelist = []\n",
    "            for k in label0ruledict.values():\n",
    "                label0rulelist +=k\n",
    "            \n",
    "            for k in label1ruledict.values():\n",
    "                label1rulelist +=k\n",
    "            \n",
    "            for s in label0rulelist:\n",
    "                if s in label1rulelist:\n",
    "                    print(s)     \n",
    "\n",
    "            ruleresult = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold'+str(threshold)\n",
    "            try:\n",
    "                os.makedirs(ruleresult)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            json_name = ruleresult+'/prefix_'+str(prefixlength)+'_label1_rnd_'+str(rndst)+'.json'\n",
    "            with open(json_name,'w') as json_file:\n",
    "                json.dump(label1ruledict,json_file)\n",
    "\n",
    "            json_name = ruleresult+'/prefix_'+str(prefixlength)+'_label0_rnd_'+str(rndst)+'.json'\n",
    "            print(json_name)\n",
    "            with open(json_name,'w') as json_file:\n",
    "                json.dump(label0ruledict,json_file)\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    way3()\n",
    "    #playsound('../Yattong+edited+version.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#from playsound import playsound\n",
    "\n",
    "for prefix in range(5,41, 5):\n",
    "    for rndst in [0,1,2,3,4]:\n",
    "        ruledict = {'Label_1':{},'Label_0':{}}\n",
    "        threshold = 0.9\n",
    "        label1 = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold'+str(threshold)+'/prefix_'+str(prefix)+'_label1_rnd_'+str(rndst)+'.json'\n",
    "        \n",
    "        with open(label1,'r') as f:\n",
    "            label1 = json.load(f)\n",
    "        \n",
    "        supportlevelist = sorted(list(label1.keys()),key=len)\n",
    "        label1rules = []\n",
    "        \n",
    "        for x in supportlevelist:\n",
    "            label1rules +=label1[x]\n",
    "            ruledict['Label_1'][x] = label1[x]\n",
    "\n",
    "        label0 = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold'+str(threshold)+'/prefix_'+str(prefix)+'_label0_rnd_'+str(rndst)+'.json'\n",
    "        with open(label0,'r') as f:\n",
    "            label0 = json.load(f)\n",
    "        \n",
    "        supportlevelist = sorted(list(label0.keys()),key=len)\n",
    "        label0rules = []\n",
    "        \n",
    "        for x in supportlevelist:\n",
    "            label0rules +=label0[x]\n",
    "            ruledict['Label_0'][x] = label0[x]\n",
    "\n",
    "        savefilename = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold'+str(threshold)+'/Rule_prefix'+str(prefix)+'_rnd'+str(rndst)+'.json'\n",
    "        with open(savefilename,'w') as f:\n",
    "            json.dump(ruledict,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comporison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix :5\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :10\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :15\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :20\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :25\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :30\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :35\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n",
      "Prefix :40\n",
      "0.1 4\n",
      "0.2 4\n",
      "0.3 4\n",
      "0.4 4\n",
      "0.5 4\n",
      "0.6 4\n",
      "0.7 4\n",
      "0.8 4\n",
      "0.9 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "# from playsound import playsound\n",
    "\n",
    "def loadrule(prefix,rndst,threshold):\n",
    "    filename = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold'+str(threshold)+'/Rule_prefix'+str(prefix)+'_rnd'+str(rndst)+'.json'\n",
    "    with open(filename,'r') as f:\n",
    "        rules = json.load(f)\n",
    "    label1rulepre = rules['Label_1']\n",
    "    label0rulepre = rules['Label_0']\n",
    "\n",
    "    label1rule =[]\n",
    "    for key in label1rulepre.keys():\n",
    "        for rule in label1rulepre[key]:\n",
    "            label1rule.append(rule)\n",
    "    \n",
    "    label0rule =[]\n",
    "    for key in label0rulepre.keys():\n",
    "        for rule in label0rulepre[key]:\n",
    "            label0rule.append(rule)\n",
    "    rules = {'Label_1':label1rule,'Label_0':label0rule}\n",
    "    return rules\n",
    "\n",
    "def loadrule2(prefix,rndst,threshold):\n",
    "    \n",
    "    filename = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/threshold'+str(threshold)+'/Rule_prefix'+str(prefix)+'_rnd'+str(rndst)+'.json'\n",
    "    with open(filename,'r') as f:\n",
    "        rules = json.load(f)\n",
    "    return rules\n",
    "\n",
    "\n",
    "def thirdmethod(testset,rules,score_thr):\n",
    "    df = pd.read_csv(testset)\n",
    "    label1 = 'Label_1'\n",
    "    label0 = 'Label_0'\n",
    "    label1rules = rules[label1]\n",
    "    label0rules = rules[label0]\n",
    "    try:\n",
    "        df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "    except:\n",
    "        pass\n",
    "    caseidlist = list(df['Case ID'])\n",
    "    label1list = list(df['Label_1'])\n",
    "    label0list = list(df['Label_0'])\n",
    "\n",
    "    y_true = {}\n",
    "    for pos,case in enumerate(caseidlist):\n",
    "        if label1list[pos] ==1:\n",
    "            y_true[case] = [1]\n",
    "        else:\n",
    "            y_true[case] = [0]\n",
    "\n",
    "    for pos,k in enumerate(label1rules):\n",
    "        label1rules[pos] = k.split('/')\n",
    "\n",
    "    for pos,k in enumerate(label0rules):\n",
    "        label0rules[pos] = k.split('/')\n",
    "\n",
    "    df = df.drop(columns=['Case ID'],axis=1)\n",
    "    cols = df.columns.values\n",
    "    colindexing={}\n",
    "    for c in cols:\n",
    "        for pos,indexing in enumerate(list(df.loc[:,c])):\n",
    "            if indexing ==1:\n",
    "                if caseidlist[pos] not in colindexing.keys():\n",
    "                    colindexing[caseidlist[pos]] = [c]\n",
    "                else:\n",
    "                    colindexing[caseidlist[pos]].append(c)\n",
    "    \n",
    "    satisfyingrule={} #key = caseid, item = list [0] = # of satisfying label0 rules [1] = # of satisfying label0 rules\n",
    "    for caseid in colindexing.keys():\n",
    "        satisfyingrule[caseid] = [0]\n",
    "        for rule in label0rules:\n",
    "            result = all(elem in colindexing[caseid] for elem in rule)\n",
    "            if result:\n",
    "                satisfyingrule[caseid][0] +=1/len(label0rules)\n",
    "    \n",
    "    for case in list(satisfyingrule.keys()):\n",
    "        if satisfyingrule[case][0]>=score_thr:\n",
    "            y_true[case].append(0)\n",
    "        else:\n",
    "            y_true[case].append(1)\n",
    "\n",
    "    true_y = list(pd.DataFrame(y_true).T[0])\n",
    "    predict_y = list(pd.DataFrame(y_true).T[1])\n",
    "\n",
    "    result = classification_report(true_y,predict_y,target_names=['Label 0','Label 1'],output_dict=True)\n",
    "    return result\n",
    "\n",
    "def fourthmethod(testset,rules,score_thr):\n",
    "    df = pd.read_csv(testset)\n",
    "    label1 = 'Label_1'\n",
    "    label0 = 'Label_0'\n",
    "    label1rules = rules[label1]\n",
    "    label0rules = rules[label0]\n",
    "    try:\n",
    "        df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "    except:\n",
    "        pass\n",
    "    caseidlist = list(df['Case ID'])\n",
    "    label1list = list(df['Label_1'])\n",
    "    label0list = list(df['Label_0'])\n",
    "\n",
    "    y_true = {}\n",
    "    for pos,case in enumerate(caseidlist):\n",
    "        if label1list[pos] ==1:\n",
    "            y_true[case] = [1]\n",
    "        else:\n",
    "            y_true[case] = [0]\n",
    "\n",
    "    for pos,k in enumerate(label1rules):\n",
    "        label1rules[pos] = k.split('/')\n",
    "\n",
    "    for pos,k in enumerate(label0rules):\n",
    "        label0rules[pos] = k.split('/')\n",
    "\n",
    "    df = df.drop(columns=['Case ID'],axis=1)\n",
    "    cols = df.columns.values\n",
    "    colindexing={}\n",
    "    for c in cols:\n",
    "        for pos,indexing in enumerate(list(df.loc[:,c])):\n",
    "            if indexing ==1:\n",
    "                if caseidlist[pos] not in colindexing.keys():\n",
    "                    colindexing[caseidlist[pos]] = [c]\n",
    "                else:\n",
    "                    colindexing[caseidlist[pos]].append(c)\n",
    "    \n",
    "    satisfyingrule={} #key = caseid, item = list [0] = # of satisfying label0 rules [1] = # of satisfying label0 rules\n",
    "    for caseid in colindexing.keys():\n",
    "        satisfyingrule[caseid] = [0]\n",
    "        for rule in label1rules:\n",
    "            result = all(elem in colindexing[caseid] for elem in rule)\n",
    "            if result:\n",
    "                satisfyingrule[caseid][0] +=1/len(label1rules)\n",
    "    \n",
    "    for case in list(satisfyingrule.keys()):\n",
    "        if satisfyingrule[case][0]>=score_thr:\n",
    "            y_true[case].append(1)\n",
    "        else:\n",
    "            y_true[case].append(0)\n",
    "\n",
    "    true_y = list(pd.DataFrame(y_true).T[0])\n",
    "    predict_y = list(pd.DataFrame(y_true).T[1])\n",
    "\n",
    "    result = classification_report(true_y,predict_y,target_names=['Label 0','Label 1'],output_dict=True)\n",
    "    return result\n",
    "\n",
    "def fifthmethod(testset,rules): #Use loadrule2 function\n",
    "    df = pd.read_csv(testset)\n",
    "    label1 = 'Label_1'\n",
    "    label0 = 'Label_0'\n",
    "    label1rules = rules[label1]\n",
    "    label0rules = rules[label0]\n",
    "\n",
    "    if len(label1rules) ==0 and len(label0rules)==0:\n",
    "        result = {'Label 1':{'precision':0,'recall':0,'f1-score':0,'support':0},'Label 0':{'precision':0,'recall':0,'f1-score':0,'support':0}}\n",
    "        return result\n",
    "    try:\n",
    "        df = df.rename(columns={'Label_1.0':'Label_1','Label_0.0':'Label_0'})\n",
    "    except:\n",
    "        pass\n",
    "    caseidlist = list(df['Case ID'])\n",
    "    label1list = list(df['Label_1'])\n",
    "    label0list = list(df['Label_0'])\n",
    "\n",
    "    y_true = {}\n",
    "    for pos,case in enumerate(caseidlist):\n",
    "        if label1list[pos] ==1:\n",
    "            y_true[case] = [1]\n",
    "        else:\n",
    "            y_true[case] = [0]\n",
    "\n",
    "    for subrule in label1rules.values():\n",
    "        for pos,smallrule in enumerate(subrule):\n",
    "            subrule[pos] = smallrule.split('/')\n",
    "    \n",
    "    for subrule in label0rules.values():\n",
    "        for pos,smallrule in enumerate(subrule):\n",
    "            subrule[pos] = smallrule.split('/')\n",
    "    \n",
    "    label0ruleweighted = 0\n",
    "    label1ruleweighted = 0\n",
    "    for supplevel in label0rules.keys():\n",
    "        subrule = label0rules[supplevel]\n",
    "        label0ruleweighted += float(supplevel)*len(subrule)    \n",
    "    for supplevel in label1rules.keys():\n",
    "        subrule = label1rules[supplevel]\n",
    "        label1ruleweighted += float(supplevel)*len(subrule)\n",
    "\n",
    "    df = df.drop(columns=['Case ID'],axis=1)\n",
    "    cols = df.columns.values\n",
    "    colindexing={}\n",
    "    for c in cols:\n",
    "        for pos,indexing in enumerate(list(df.loc[:,c])):\n",
    "            if indexing ==1:\n",
    "                if caseidlist[pos] not in colindexing.keys():\n",
    "                    colindexing[caseidlist[pos]] = [c]\n",
    "                else:\n",
    "                    colindexing[caseidlist[pos]].append(c)\n",
    "    \n",
    "    satisfyingrule={} #key = caseid, item = list [0] = # of satisfying label0 rules [1] = # of satisfying label0 rules\n",
    "    for caseid in colindexing.keys():\n",
    "        satisfyingrule[caseid] = [0,0]\n",
    "        for supplevel in label0rules.keys():\n",
    "            subrule = label0rules[supplevel]\n",
    "            for smallrule in subrule:\n",
    "                result = all(elem in colindexing[caseid] for elem in smallrule)\n",
    "                if result:\n",
    "                    satisfyingrule[caseid][0] +=float(supplevel)/label0ruleweighted\n",
    "\n",
    "        for supplevel in label1rules.keys():\n",
    "            subrule = label1rules[supplevel]\n",
    "            for smallrule in subrule:\n",
    "                result = all(elem in colindexing[caseid] for elem in smallrule)\n",
    "                if result:\n",
    "                    satisfyingrule[caseid][1] +=float(supplevel)/label1ruleweighted\n",
    "    \n",
    "        if satisfyingrule[caseid][0] > satisfyingrule[caseid][1]:\n",
    "            y_true[caseid].append(0)\n",
    "        elif satisfyingrule[caseid][0] < satisfyingrule[caseid][1]:\n",
    "            y_true[caseid].append(1)\n",
    "        else:\n",
    "            del(y_true[caseid])\n",
    "\n",
    "    true_y = list(pd.DataFrame(y_true).T[0])\n",
    "    predict_y = list(pd.DataFrame(y_true).T[1])\n",
    "\n",
    "    result = classification_report(true_y,predict_y,target_names=['Label 0','Label 1'],output_dict=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    for prefix in range(5,41, 5):\n",
    "        print(\"Prefix :%s\"%(prefix))\n",
    "        resultdict={}\n",
    "        resultdict['Label 0'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        resultdict['Label 1'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        \n",
    "        resultdict3={}\n",
    "        resultdict3['Label 0'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        resultdict3['Label 1'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        \n",
    "        resultdict5={}\n",
    "        resultdict5['Label 0'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "        resultdict5['Label 1'] ={'precision':[],'recall':[],'f1-score':[],'support':[]}\n",
    "\n",
    "        for score_thr in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "            for rndst in [0,1,2,3,4]:\n",
    "                for threshold in [0.9]:\n",
    "                    testset = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/RIPPER/prefix'+str(prefix)+'/test_rndst'+str(rndst)+'.csv'\n",
    "                    rules=loadrule(prefix,rndst,threshold)\n",
    "                    rules2=loadrule2(prefix, rndst,threshold)\n",
    "                    #result = fourthmethod(testset,rules,score_thr)\n",
    "                    #result3=thirdmethod(testset, rules, score_thr)\n",
    "                    result5=fifthmethod(testset, rules2)\n",
    "                    \"\"\"\"\n",
    "                    resultdict['Label 0']['precision'].append(result['Label 0']['precision'])\n",
    "                    resultdict['Label 0']['recall'].append(result['Label 0']['recall'])\n",
    "                    resultdict['Label 0']['f1-score'].append(result['Label 0']['f1-score'])\n",
    "                    resultdict['Label 0']['support'].append(result['Label 0']['support'])\n",
    "                    resultdict['Label 1']['precision'].append(result['Label 1']['precision'])\n",
    "                    resultdict['Label 1']['recall'].append(result['Label 1']['recall'])\n",
    "                    resultdict['Label 1']['f1-score'].append(result['Label 1']['f1-score'])\n",
    "                    resultdict['Label 1']['support'].append(result['Label 1']['support'])\n",
    "                    \n",
    "                    resultdict3['Label 0']['precision'].append(result3['Label 0']['precision'])\n",
    "                    resultdict3['Label 0']['recall'].append(result3['Label 0']['recall'])\n",
    "                    resultdict3['Label 0']['f1-score'].append(result3['Label 0']['f1-score'])\n",
    "                    resultdict3['Label 0']['support'].append(result3['Label 0']['support'])\n",
    "                    resultdict3['Label 1']['precision'].append(result3['Label 1']['precision'])\n",
    "                    resultdict3['Label 1']['recall'].append(result3['Label 1']['recall'])\n",
    "                    resultdict3['Label 1']['f1-score'].append(result3['Label 1']['f1-score'])\n",
    "                    resultdict3['Label 1']['support'].append(result3['Label 1']['support'])\n",
    "                    \"\"\"\n",
    "                    resultdict5['Label 0']['precision'].append(result5['Label 0']['precision'])\n",
    "                    resultdict5['Label 0']['recall'].append(result5['Label 0']['recall'])\n",
    "                    resultdict5['Label 0']['f1-score'].append(result5['Label 0']['f1-score'])\n",
    "                    resultdict5['Label 0']['support'].append(result5['Label 0']['support'])\n",
    "                    resultdict5['Label 1']['precision'].append(result5['Label 1']['precision'])\n",
    "                    resultdict5['Label 1']['recall'].append(result5['Label 1']['recall'])\n",
    "                    resultdict5['Label 1']['f1-score'].append(result5['Label 1']['f1-score'])\n",
    "                    resultdict5['Label 1']['support'].append(result5['Label 1']['support'])\n",
    "            \"\"\"\"\n",
    "            for pre in resultdict.keys():\n",
    "                for col in resultdict[pre].keys():\n",
    "                    resultdict[pre][col] = [np.mean(resultdict[pre][col]),np.std(resultdict[pre][col])]\n",
    "            resultdir = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/fourthmethod/score_thr'+str(score_thr)\n",
    "            try:\n",
    "                os.makedirs(resultdir)\n",
    "            except:\n",
    "                pass\n",
    "            jsonname = resultdir+'/prefix'+str(prefix)+'result.json'\n",
    "            print(score_thr, rndst)\n",
    "            with open(jsonname ,'w') as f:\n",
    "                json.dump(resultdict,f)\n",
    "                \n",
    "            for pre in resultdict3.keys():\n",
    "                for col in resultdict3[pre].keys():\n",
    "                    resultdict3[pre][col] = [np.mean(resultdict3[pre][col]),np.std(resultdict3[pre][col])]\n",
    "            resultdir3 = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/thirdmethod/score_thr'+str(score_thr)\n",
    "            try:\n",
    "                os.makedirs(resultdir3)\n",
    "            except:\n",
    "                pass\n",
    "            jsonname3 = resultdir3+'/prefix'+str(prefix)+'result.json'\n",
    "            print(score_thr, rndst)\n",
    "            with open(jsonname3 ,'w') as f:\n",
    "                json.dump(resultdict3,f)\n",
    "            \"\"\"    \n",
    "            for pre in resultdict5.keys():\n",
    "                for col in resultdict5[pre].keys():\n",
    "                    resultdict5[pre][col] = [np.mean(resultdict5[pre][col]),np.std(resultdict5[pre][col])]\n",
    "            resultdir5 = r'C:/Users/Dias/Desktop/data/dataset/bpic2012/ruleresult/way3/fifthmethod/score_thr'+str(score_thr)\n",
    "            try:\n",
    "                os.makedirs(resultdir5)\n",
    "            except:\n",
    "                pass\n",
    "            jsonname5 = resultdir5+'/prefix'+str(prefix)+'result.json'\n",
    "            print(score_thr, rndst)\n",
    "            with open(jsonname5 ,'w') as f:\n",
    "                json.dump(resultdict5,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conviction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 210663.69it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 798014.56it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 211317.61it/s]\n",
      "211it [00:00, 678679.56it/s]\n",
      "211it [00:00, 230949.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  5 Rnd 0\n",
      "31\n",
      "Prefix  5 Rnd 1\n",
      "31\n",
      "Prefix  5 Rnd 2\n",
      "31\n",
      "Prefix  5 Rnd 3\n",
      "31\n",
      "Prefix  5 Rnd 4\n",
      "31\n",
      "Prefix  10 Rnd 0\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 206823.59it/s]\n",
      "211it [00:00, 511855.49it/s]\n",
      "211it [00:00, 211620.79it/s]\n",
      "211it [00:00, 212026.39it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 376274.72it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 404293.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  10 Rnd 1\n",
      "31\n",
      "Prefix  10 Rnd 2\n",
      "31\n",
      "Prefix  10 Rnd 3\n",
      "31\n",
      "Prefix  10 Rnd 4\n",
      "31\n",
      "Prefix  15 Rnd 0\n",
      "31\n",
      "Prefix  15 Rnd 1\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 197280.01it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 211620.79it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 211570.20it/s]\n",
      "211it [00:00, 209864.39it/s]\n",
      "211it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  15 Rnd 2\n",
      "31\n",
      "Prefix  15 Rnd 3\n",
      "31\n",
      "Prefix  15 Rnd 4\n",
      "31\n",
      "Prefix  20 Rnd 0\n",
      "31\n",
      "Prefix  20 Rnd 1\n",
      "31\n",
      "Prefix  20 Rnd 2\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 549688.29it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 516938.17it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 211570.20it/s]\n",
      "211it [00:00, 210213.34it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 687110.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  20 Rnd 3\n",
      "31\n",
      "Prefix  20 Rnd 4\n",
      "31\n",
      "Prefix  25 Rnd 0\n",
      "31\n",
      "Prefix  25 Rnd 1\n",
      "31\n",
      "Prefix  25 Rnd 2\n",
      "31\n",
      "Prefix  25 Rnd 3\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 211368.08it/s]\n",
      "211it [00:00, 345163.08it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 6654121.38it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 1311108.36it/s]\n",
      "211it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  25 Rnd 4\n",
      "31\n",
      "Prefix  30 Rnd 0\n",
      "31\n",
      "Prefix  30 Rnd 1\n",
      "31\n",
      "Prefix  30 Rnd 2\n",
      "31\n",
      "Prefix  30 Rnd 3\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 211418.57it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 267290.29it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 171777.59it/s]\n",
      "211it [00:00, 211570.20it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 222026.63it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  30 Rnd 4\n",
      "31\n",
      "Prefix  35 Rnd 0\n",
      "31\n",
      "Prefix  35 Rnd 1\n",
      "31\n",
      "Prefix  35 Rnd 2\n",
      "31\n",
      "Prefix  35 Rnd 3\n",
      "31\n",
      "Prefix  35 Rnd 4\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 648825.62it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 207502.50it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 155563.04it/s]\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, 735048.29it/s]\n",
      "211it [00:00, 211519.63it/s]\n",
      "211it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix  40 Rnd 0\n",
      "31\n",
      "Prefix  40 Rnd 1\n",
      "31\n",
      "Prefix  40 Rnd 2\n",
      "31\n",
      "Prefix  40 Rnd 3\n",
      "31\n",
      "Prefix  40 Rnd 4\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "211it [00:00, ?it/s]\n",
      "211it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import ast\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "#from playsound import playsound\n",
    "from tqdm import tqdm\n",
    "\n",
    "def ersecommonrule(both_label):\n",
    "    label0df = both_label[0]\n",
    "    label1df = both_label[1]\n",
    "    label0rule = []\n",
    "    label1rule = []\n",
    "\n",
    "    for x in label0df['antecedents']:\n",
    "        x = sorted(ast.literal_eval(x))\n",
    "        label0rule.append(str(x))\n",
    "\n",
    "    for x in label1df['antecedents']:\n",
    "        x = sorted(ast.literal_eval(x))\n",
    "        label1rule.append(str(x))\n",
    "    label0rules = frozenset(label0rule)\n",
    "    label1rules = frozenset(label1rule)\n",
    "    commonrule = label0rules.intersection(label1rules)\n",
    "    print(len(commonrule))\n",
    "    unqiuelabel0 = [pos for pos,x in tqdm(enumerate(label0rule)) if x not in commonrule]\n",
    "    unqiuelabel1 = [pos for pos,x in tqdm(enumerate(label1rule)) if x not in commonrule]\n",
    "    \n",
    "    return label0df.iloc[unqiuelabel0,:], label1df.iloc[unqiuelabel1,:]\n",
    "\n",
    "def summarizerule(ndf):\n",
    "    if len(ndf) ==0:\n",
    "        return {}\n",
    "    # ndf['conviction dist'] =abs(ndf['conviction'] - 1)\n",
    "    groups = ndf.groupby('antecedents')\n",
    "    rulebeforeclustering=[]\n",
    "    for case, group in groups:\n",
    "        group = group.sort_values(by='conviction',ascending=False)\n",
    "        group = group.reset_index(drop=True)\n",
    "        rulebeforeclustering.append(group.iloc[0,:])\n",
    "        \n",
    "    ndf = pd.DataFrame(rulebeforeclustering).reset_index(drop=True)\n",
    "    allelement = set()\n",
    "    for x in list(ndf['antecedents']):\n",
    "        x = ast.literal_eval(x)\n",
    "        for k in x:\n",
    "            allelement.add(k)\n",
    "    \n",
    "    for pos,x in enumerate(list(ndf['antecedents'])):\n",
    "        x = ast.literal_eval(x)\n",
    "        for k in allelement:\n",
    "            if k in x:\n",
    "                ndf.loc[pos,k] =1\n",
    "            else:\n",
    "                ndf.loc[pos,k] =0\n",
    "\n",
    "    try:\n",
    "        model = KMeans(n_clusters=20)\n",
    "        model.fit(ndf.loc[:,allelement])\n",
    "\n",
    "        y_predict = model.fit_predict(ndf.loc[:,allelement])\n",
    "\n",
    "        ndf['cluster'] = y_predict\n",
    "        groups = ndf.groupby('cluster')\n",
    "        topsupport = []\n",
    "        for case, group in groups:\n",
    "            group = group.sort_values(by='support',ascending=False)\n",
    "            group = group.reset_index(drop=True)\n",
    "            topsupport.append(group.iloc[0,:])\n",
    "        ndf = pd.DataFrame(topsupport)\n",
    "    except:\n",
    "        pass\n",
    "    ndf = ndf.reset_index(drop=True)\n",
    "    supportlist= []\n",
    "    for x in ndf['support']:\n",
    "        supportlist.append(int(x*10)/10)\n",
    "    ndf['support'] =supportlist\n",
    "    data ={}\n",
    "    for pos,x in enumerate(list(ndf['antecedents'])):\n",
    "        x = ast.literal_eval(x)\n",
    "        rule = '/'.join(x)\n",
    "        supp = ndf.loc[pos,'support']\n",
    "        if supp not in list(data.keys()):\n",
    "            data[ndf.loc[pos,'support']] = [rule]\n",
    "        else:\n",
    "            data[ndf.loc[pos,'support']].append(rule)\n",
    "    return data\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    for prefix in range(5,41,5):\n",
    "        for rndst in range(0,5):\n",
    "            label_ruledf = []\n",
    "            print('Prefix ', prefix, \"Rnd\", rndst)\n",
    "            for label in [0,1]:\n",
    "                data=[]\n",
    "                for supp in [0.9]:\n",
    "                    df =  pd.read_csv(r'C:/Users/Dias/Desktop/data/dataset/bpic2012/RIPPER/prefix'+str(prefix)+'/threshold0.9/support_'+str(supp)+'/label'+str(label)+'result_rndst'+str(rndst)+'.csv')                    \n",
    "                    # if len(df) !=0:\n",
    "                    data.append(df)\n",
    "\n",
    "                ndf = pd.concat(data)\n",
    "                label_ruledf.append(ndf)\n",
    "            data={}\n",
    "            label0df, label1df = ersecommonrule(label_ruledf)\n",
    "            data['Label_0'] = summarizerule(label0df)\n",
    "            data['Label_1'] = summarizerule(label1df)\n",
    "            rulefilename = r'C:/Users/Dias/Desktop/data/dataset/bpic2012//ruleresult/way3/threshold0.9/Summarized_Rule_prefix'+str(prefix)+'_rnd'+str(rndst)+'.json'\n",
    "            with open(rulefilename,'w') as f:\n",
    "                json.dump(data,f)\n",
    "    #playsound('../Yattong+edited+version.mp3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BPIC2012_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
